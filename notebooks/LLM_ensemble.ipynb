{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM ensembling\n",
    "\n",
    "This simple ensemble method works as follows:\n",
    "1. First train $N$ different models ($N=2$ in our case), each of which takes neural activty as input and ouputs a single best candidate sentence.\n",
    "2. Use a Large Language Model (LLM) to choose the highest LLM score among the $N$ candidate sentences.\n",
    "\n",
    "We demonstrate step 2 in this notebook. Note that\n",
    "- The inputs to this ensembling step only requires candidate sentences from each model.\n",
    "- There are two different LLMs used in this method.\n",
    "    - First LLM is used for the language model decoding step within each model (OPT6.7B in our case).\n",
    "    - Second LLM is used for ensembling (Llama2-7B-chat model in our case)\n",
    "- While we have not experimented with these model choices extensively, our initial experiments suggest that models that are optimized for conversations (such as Llama2 chat) might work well for the ensembling step (i.e. the second LLM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CURL_CA_BUNDLE'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from NeuralDecoder.neuralDecoder.utils.lmDecoderUtils import _cer_and_wer as cer_and_wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the model and specify the path in the `MODEL_CACHE_DIR` below. Lllama-2-7b-chat-hf could be downloaded from Hugging Face [here](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM path, we use Llama2-7b-chat here\n",
    "MODEL_NAME = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "MODEL_CACHE_DIR = '/home/user/LLM/Llama-2-7b-chat'\n",
    "\n",
    "# True test sentences\n",
    "target_test_file = './samples/target_test.txt'\n",
    "# Decoded test sentences with 11.71% and 12.19% test WER\n",
    "model1_test_file = './samples/model1_test.txt'\n",
    "model2_test_file = './samples/model2_test.txt'\n",
    "\n",
    "# Decoded competition sentences with 11.71% and 12.19% test WER\n",
    "model1_competition_file = './samples/model1_competition.txt'\n",
    "model2_competition_file = './samples/model2_competition.txt'\n",
    "\n",
    "# Output file\n",
    "output_file = './samples/Llama2chat_ensemble.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load an LLM for ensembling\n",
    "\n",
    "We use Llama2-7B-chat model since it is optimized for dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_llm(modelName=None, cacheDir=None, device='auto', load_in_8bit=False):\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelName, cache_dir=cacheDir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(modelName, cache_dir=cacheDir,\n",
    "                                                 device_map=device, load_in_8bit=load_in_8bit)\n",
    "\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff7b1a1ca7e4e7e99bf3b2691d8ec49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n"
     ]
    }
   ],
   "source": [
    "llm_model, tokenizer = build_llm(modelName=MODEL_NAME, cacheDir=MODEL_CACHE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentence(file):\n",
    "    sentence = []\n",
    "    with open(file, 'r') as f:\n",
    "        for s in f.readlines():\n",
    "            s = s.strip('\\n')\n",
    "            sentence.append(s)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_test = load_sentence(target_test_file)\n",
    "model1_test = load_sentence(model1_test_file)\n",
    "model2_test = load_sentence(model2_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate LLM score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_llm_score(sentences):\n",
    "    scores = []\n",
    "\n",
    "    for s in range(len(sentences)):\n",
    "        sentence = sentences[s]\n",
    "        inputs = tokenizer(sentence, return_tensors='pt', padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = llm_model(**inputs)\n",
    "            logProbs = torch.nn.functional.log_softmax(outputs['logits'].float(), -1).numpy()\n",
    "        B, T, _ = logProbs.shape\n",
    "        for i in range(B):\n",
    "            n_tokens = np.sum(inputs['attention_mask'][i].numpy())\n",
    "            newLMScore = 0.\n",
    "            for j in range(1, n_tokens):\n",
    "                newLMScore += logProbs[i, j - 1, inputs['input_ids'][i, j].numpy()]\n",
    "        scores.append(newLMScore)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_test_score = cal_llm_score(model1_test)\n",
    "model2_test_score = cal_llm_score(model2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick a sentence with the highest LLM score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_sentence(sentence1, sentence2, score1, score2):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sentence1 (list): decoded sentences using model1\n",
    "        sentence2 (list): decoded sentences using model2\n",
    "        score1 (list): LLM score for sentence1\n",
    "        score2 (list): LLM score for sentence2\n",
    "\n",
    "    Returns:\n",
    "        decoded (list): sentences picked based on LLM score\n",
    "        pick (list): which sentence the output comes from, \"model 1\" or \"model 2\", \" \" for sentences same in 2 models\n",
    "    \"\"\"\n",
    "    \n",
    "    decoded = []\n",
    "    pick = []\n",
    "\n",
    "    assert len(sentence1)==len(sentence2)\n",
    "    for s in range(len(sentence1)):\n",
    "        if score1[s] > score2[s]:\n",
    "            decoded.append(sentence1[s])\n",
    "            pick.append(\"model 1\")\n",
    "        else:\n",
    "            decoded.append(sentence2[s])\n",
    "            if score1[s] == score2[s]:\n",
    "                pick.append(\" \")\n",
    "            else:\n",
    "                pick.append(\"model 2\")\n",
    "\n",
    "    return decoded, pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test output\n",
      "#Outputs picked from model 1: 110\n",
      "#Outputs picked from model 2: 91\n"
     ]
    }
   ],
   "source": [
    "decoded_test, pick_test = pick_sentence(model1_test, model2_test, model1_test_score, model2_test_score)\n",
    "\n",
    "print(\"Test output\")\n",
    "print(\"#Outputs picked from model 1:\", sum(np.array(pick_test)==\"model 1\"))\n",
    "print(\"#Outputs picked from model 2:\", sum(np.array(pick_test)==\"model 2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_output(index):\n",
    "    separator = '-' * 100 + '\\n'\n",
    "    output = ''\n",
    "    sentence_num = index + 1\n",
    "    target_output = target_test[index]\n",
    "    model_outputs = [\n",
    "        {'model_name': 'Model 1', 'output': model1_test[index], 'score': model1_test_score[index]},\n",
    "        {'model_name': 'Model 2', 'output': model2_test[index], 'score': model2_test_score[index]},\n",
    "    ]\n",
    "    final_output = decoded_test[index]\n",
    "    output += separator\n",
    "    output += f'Sentence: {sentence_num}\\n'\n",
    "    output += separator\n",
    "    output += f'Target output : {target_output}\\n'\n",
    "    output += separator\n",
    "    for model_output in model_outputs:\n",
    "        model_name = model_output['model_name']\n",
    "        text = model_output['output']\n",
    "        score = model_output['score']\n",
    "        output += f'{model_name} output: {text:<50} (LLM score: {score:.1f})\\n'\n",
    "    output += separator\n",
    "    output += f'Final output  : {final_output}\\n'\n",
    "    output += separator\n",
    "    output += '\\n'\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence: 152\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target output : i'm away from my other son during those hours\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model 1 output: i'm away from my other son during those hours      (LLM score: -60.7)\n",
      "Model 2 output: i'm really from my other son doing those hours     (LLM score: -78.7)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Final output  : i'm away from my other son during those hours\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# An example where model 1 output was correct and LLM correctly chose model 1\n",
    "format_output(151)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence: 331\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target output : i really would like to see them do well\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model 1 output: we would like to see them too well                 (LLM score: -45.6)\n",
      "Model 2 output: i really would like to see them do well            (LLM score: -39.6)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Final output  : i really would like to see them do well\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# An example where model 2 output was correct and LLM correctly chose model 2\n",
    "format_output(330)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence: 271\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target output : can you speak more about this system of treasury decentralization\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model 1 output: then you make more about this system of treasury us inflation (LLM score: -83.0)\n",
      "Model 2 output: then you pick more about this system of treasury decentralization (LLM score: -74.6)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Final output  : then you pick more about this system of treasury decentralization\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# An example where neither models were correct, but LLM chose the sentence with lower WER\n",
    "format_output(270)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence: 338\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target output : anything like that we participated\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model 1 output: anything like that we participated                 (LLM score: -38.6)\n",
      "Model 2 output: anything like that we must abide                   (LLM score: -38.4)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Final output  : anything like that we must abide\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# An example where model 1 was correct but LLM chose model 2 (ensembling lead to a worse result)\n",
    "format_output(337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, wer1 = cer_and_wer(model1_test, target_test, outputType='speech_sil', returnCI=False)\n",
    "_, wer2 = cer_and_wer(model2_test, target_test, outputType='speech_sil', returnCI=False)\n",
    "_, ensemble_wer = cer_and_wer(decoded_test, target_test, outputType='speech_sil', returnCI=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER before LLM ensembling: 11.71% (model 1), 12.19% (model 2)\n",
      "WER after LLM ensembling: 11.30%\n"
     ]
    }
   ],
   "source": [
    "print (f\"WER before LLM ensembling: {(wer1*100):.2f}% (model 1), {(wer2*100):.2f}% (model 2)\")\n",
    "print (f\"WER after LLM ensembling: {(ensemble_wer*100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competition data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load competition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_competition = load_sentence(model1_competition_file)\n",
    "model2_competition = load_sentence(model2_competition_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate LLM score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_competition_score = cal_llm_score(model1_competition)\n",
    "model2_competition_score = cal_llm_score(model2_competition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick a sentence with the highest LLM score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Competition output\n",
      "#Outputs picked from model 1: 186\n",
      "#Outputs picked from model 2: 139\n"
     ]
    }
   ],
   "source": [
    "decoded_competition, pick_competition = pick_sentence(model1_competition, model2_competition, model1_competition_score, model2_competition_score)\n",
    "\n",
    "\n",
    "print(\"Competition output\")\n",
    "print(\"#Outputs picked from model 1:\", sum(np.array(pick_competition)==\"model 1\"))\n",
    "print(\"#Outputs picked from model 2:\", sum(np.array(pick_competition)==\"model 2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file, 'w') as f:\n",
    "    for x in range(len(decoded_competition)):\n",
    "        f.write(decoded_competition[x]+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speechBCI_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
